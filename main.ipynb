{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0gkjD_yyaYI",
        "outputId": "cfd4c692-d12c-4cea-abd4-e8c60a7a5bf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJVIvTK4x0qe",
        "outputId": "120cb014-87b0-4b35-e619-75f7a5245298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "The current device is Tesla T4\n",
            "\n",
            " Epoch 1 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.695\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 2 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 3 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.695\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 4 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 5 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 6 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 7 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 8 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.695\n",
            "Validation Loss: 0.695\n",
            "\n",
            " Epoch 9 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 10 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.693\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import random\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.optim import SGD\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds=[]\n",
        "  \n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  total_preds = []\n",
        "\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  set_seed(5)\n",
        "\n",
        "  # import train and test file\n",
        "  '''\n",
        "  we are only interested in the 'tweet' and 'sarcastic' columns in the training data\n",
        "  '''\n",
        "  raw_df = pd.read_csv(\"modified.csv\")\n",
        "  df = raw_df.loc[:, ['tweet', 'sarcastic']]\n",
        "  test_df = pd.read_csv(\"task_A_En_test.csv\")\n",
        "\n",
        "  # preprocess the data\n",
        "  '''\n",
        "  we found that there were emojis that were causing some issues, so to be safe we converted the tweets column to of type string\n",
        "  '''\n",
        "  df['tweet'] = df['tweet'].astype(str)\n",
        "\n",
        "  # data split\n",
        "  '''\n",
        "  splitting the data into 90% training and 10% validation\n",
        "  '''\n",
        "  train_text, valid_text, train_labels, valid_labels = train_test_split(df['tweet'], df['sarcastic'], random_state=2018, test_size=0.2, stratify=df['sarcastic'])\n",
        "\n",
        "  # importing the BERT model / BERT tokenizer\n",
        "  '''\n",
        "  importing BERT model and BERT tokenizer\n",
        "  '''\n",
        "  bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "  tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  # TODO: label this section\n",
        "  max_seq_len = 25\n",
        "\n",
        "  # tokenize data\n",
        "  '''\n",
        "  tokenizing the training and validation data\n",
        "  '''\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "      valid_text.tolist(),\n",
        "      max_length = max_seq_len,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_df['text'].tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "  # converting integer sequences to tensors\n",
        "  '''\n",
        "  converting integer sequences to tensors\n",
        "  '''\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(valid_labels.tolist())\n",
        "\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_df['sarcastic'].tolist())\n",
        "\n",
        "  batch_size = 32\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)                              # wrap tensors\n",
        "  train_sampler = RandomSampler(train_data)                                               # used for sampling in training\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # dataloader\n",
        "\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)                                               # used for sampling in validation\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "\n",
        "  # freezing all the parameters\n",
        "  '''\n",
        "  freezing parameters\n",
        "  '''\n",
        "  for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "  # creating a model\n",
        "  '''\n",
        "  training will be faster on GPU. check if there is GPU available, if not use CPU. instantiates a model\n",
        "  '''\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          \n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print(f'The current device is {torch.cuda.get_device_name(0)}')\n",
        "  else:\n",
        "      print('CUDA is not available. Using CPU...')\n",
        "      device = torch.device(\"cpu\")\n",
        "\n",
        "  model = BERT_Arch(bert)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # computing class weights\n",
        "  '''\n",
        "  class weights are determined by distribution of training data\n",
        "  '''\n",
        "  class_wts = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "  # class weights to sensors\n",
        "  '''\n",
        "  converting class weights to sensors\n",
        "  '''\n",
        "  weights = torch.tensor(class_wts,dtype=torch.float)\n",
        "  weights = weights.to(device)\n",
        "\n",
        "  # loss function\n",
        "  '''\n",
        "  defining loss function\n",
        "  '''\n",
        "  cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "  # hyperparameters + optimizer\n",
        "  '''\n",
        "  defining hyperparameters and creating an optimizer. we decided to go with SGD as our optimizer\n",
        "  '''\n",
        "  epochs = 10\n",
        "  optimizer = SGD(model.parameters(), lr=0.009, momentum=0.9)\n",
        "\n",
        "  # training the model\n",
        "  '''\n",
        "  train the model and the best model's weights to saved_weights.pt\n",
        "  '''\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      \n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "      \n",
        "      train_loss, _ = train()\n",
        "      \n",
        "      valid_loss, _ = evaluate()\n",
        "      \n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "      \n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      \n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "  # load best weights\n",
        "  '''\n",
        "  from all the training we did, load the best weights that was saved to saved_weights.pt\n",
        "  '''\n",
        "  path = 'saved_weights.pt'\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # get predictions for the test data\n",
        "  '''\n",
        "  generate model predicted labels\n",
        "  '''\n",
        "  with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "  # output\n",
        "  '''\n",
        "  create an output file that mathces the test file. the output file will contain the model generated labels which\n",
        "  is to be used as an input for the evaluation script. output file will be called 'generated_labels.csv' and will\n",
        "  live in the directory of this file\n",
        "  '''\n",
        "  test_df_copy = test_df.copy()\n",
        "  for i in range(0, len(preds)):\n",
        "    test_df_copy.loc[i, 'sarcastic'] = preds[i]\n",
        "\n",
        "  test_df_copy.to_csv('generated_labels.csv', index=False, sep=',')"
      ]
    }
  ]
}