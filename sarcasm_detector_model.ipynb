{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itpOWhMn7oBW",
        "outputId": "3734bdaf-2d61-4f8a-c9e7-ca1dcf76ee42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOPMwdxr7vlS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KTnknhy8Mxh"
      },
      "source": [
        "<h1>Load dataset</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud1DOPiF7_D-",
        "outputId": "b91358bf-540d-4e9a-ddb7-d2ec49f46146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  sarcastic\n",
            "0  The only thing I got from college is a caffein...          1\n",
            "1  I love it when professors draw a big question ...          1\n",
            "2  Remember the hundred emails from companies whe...          1\n",
            "3  Today my pop-pop told me I was not ‚Äúforced‚Äù to...          1\n",
            "4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1 \n",
            "\n",
            "shape:  (3468, 2) \n",
            "\n",
            "class distribution:  0    0.75\n",
            "1    0.25\n",
            "Name: sarcastic, dtype: float64\n",
            "                                                text  sarcastic\n",
            "0  Size on the the Toulouse team, That pack is mo...          0\n",
            "1                                           Pinball!          0\n",
            "2  So the Scottish Government want people to get ...          1\n",
            "3  villainous pro tip : change the device name on...          0\n",
            "4                    I would date any of these men ü•∫          0\n"
          ]
        }
      ],
      "source": [
        "raw_df = pd.read_csv(\"train.En.csv\")\n",
        "df = raw_df.loc[:, ['tweet', 'sarcastic']]\n",
        "print(df.head(), \"\\n\")\n",
        "print(\"shape: \", df.shape, \"\\n\")\n",
        "print(\"class distribution: \", df['sarcastic'].value_counts(normalize = True))\n",
        "\n",
        "test_df = pd.read_csv(\"task_A_En_test.csv\")\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksfzqA-G_O_E"
      },
      "source": [
        "<h1>Preprocess the data</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oT3hYdLk_R-X"
      },
      "outputs": [],
      "source": [
        "df['tweet'] = df['tweet'].astype(str)\n",
        "test_df['text'] = test_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Splitting the training data into 90% training and 10% validation</h1>"
      ],
      "metadata": {
        "id": "ogxxGGb7mZDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gjye0_FTBD-Y"
      },
      "outputs": [],
      "source": [
        "train_text, valid_text, train_labels, valid_labels = train_test_split(df['tweet'], df['sarcastic'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=df['sarcastic'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jPT-NJ19eAd"
      },
      "source": [
        "<h1>Importing BERT Model / BERT Tokenizer</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq78j2Ak8Y1t",
        "outputId": "0feac4fb-ad34-4fcf-c04b-33d5efeb6b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlJluKmE9sh6",
        "outputId": "341fb50b-134e-44d6-ddaf-b0c85610baa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1996, 11190, 5598, 2058, 1996, 4231, 102, 0, 0, 0], [101, 1996, 4419, 5598, 2058, 1996, 4231, 1998, 5598, 2067, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# testing out how sent ids work\n",
        "text = [\"the cow jumped over the moon\", \"the fox jumped over the moon and jumped back\"]\n",
        "\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
        "\n",
        "print(sent_id)\n",
        "\n",
        "# attention masks do not capture padding values, padding is added when one input is larger than the other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "h14Xd0PO-cVc",
        "outputId": "eaea19ca-bd26-4c01-a329-3be39876beff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ify5ENQqAHQg"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tokenize data</h1>"
      ],
      "metadata": {
        "id": "PItabSonncQf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esg3JR4sAOTW",
        "outputId": "d84baac7-daf7-4dec-92f0-aa635459b0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    valid_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_df['text'].tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Convert integer sequences to tensors</h1>"
      ],
      "metadata": {
        "id": "JETPIpqLnkd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1o5WGDD6IIxO"
      },
      "outputs": [],
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(valid_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_df['sarcastic'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OsMb1i-qIyD6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y) # wrap tensors\n",
        "train_sampler = RandomSampler(train_data) # used for sampling in training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # dataloader\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data) # used for sampling in validation\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "il9bVt3_JG7k"
      },
      "outputs": [],
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wXG_jBOXJMKK"
      },
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cehVAIruJnj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8edf0fd-cdd3-4972-bca3-6396ee4a6582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Using CPU...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          \n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print(f'The current device is {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    print('CUDA is not available. Using CPU...')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn6j0UQBJsHp",
        "outputId": "166425aa-4f91-43c8-be36-7944d5a71c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.66682692 1.99855908]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_wts = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RkkOIaXgLP50"
      },
      "outputs": [],
      "source": [
        "# class weights to sensors\n",
        "weights = torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "cross_entropy = nn.NLLLoss(weight=weights) # loss function\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 20\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kD5nRIzjLWbZ"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Fine-Tuning BERT</h1>\n",
        "<p>In this cell we are saving the model predictions while iterating over batches. For each batch, we clear the previously calculated gradients and get the model's predictions for the current batch. We then calculate the loss between the actual and predicted values which is added to the total loss. We then backward pass to calculate the gradients and clip the gradients to 1.0 which helps to prevent exploding gradients. Finally, we update the parameters and append the predictions to a list of stored predictions.</p>"
      ],
      "metadata": {
        "id": "CBx3GzfSqtB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "23yYYXyRLer1"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds=[]\n",
        "  \n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "    preds = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Evaluating the model</h1>\n",
        "<p>In this cell we are computing the validation loss between the actual and predicted loss.</p>"
      ],
      "metadata": {
        "id": "5GTyS2YcrZqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "m9sEOlnpLrGL"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  total_preds = []\n",
        "\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Model training</h1>\n",
        "Here we are training, evaluating and saving the best model."
      ],
      "metadata": {
        "id": "SOxQVxCfszo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "wH8qYQ4lLuRy",
        "outputId": "3153c2e0-37d5-451a-b9b8-3b1d2eb1dd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 20\n",
            "  Batch    50  of     87.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 2 / 20\n",
            "  Batch    50  of     87.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c1a4429301b4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-ee66de69f785>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-f2338cafe1f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         )\n\u001b[0;32m-> 1020\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMD9AmLEQRad",
        "outputId": "4b998ad6-7034-4fb7-9699-96a0a2dc085e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR6NUqHlQUnq"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee-2ITm2QYc8",
        "outputId": "9f0fd466-fbac-4d3b-d390-18a9a1579393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83      1200\n",
            "           1       0.07      0.08      0.07       200\n",
            "\n",
            "    accuracy                           0.72      1400\n",
            "   macro avg       0.46      0.45      0.45      1400\n",
            "weighted avg       0.73      0.72      0.72      1400\n",
            "\n",
            "0.45324305759984296 0.723831952563186\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))\n",
        "macro_f1 = f1_score(test_y, preds, average='macro')\n",
        "weighted_f1 = f1_score(test_y, preds, average='weighted')\n",
        "print(macro_f1, weighted_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}